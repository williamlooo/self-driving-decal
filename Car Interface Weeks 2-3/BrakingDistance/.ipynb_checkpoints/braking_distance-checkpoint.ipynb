{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import controller\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 3\n",
    "MODE_DICT = {0: \"Training Neural Model\", 1: \"Demonstrate Neural Model\", \n",
    "             2: \"Demonstrate Simple Equation\", 3: \"Test Neural Model\",\n",
    "             4: \"Test Simple Equation\"}\n",
    "print(MODE_DICT[MODE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAR_MODEL = \"complex\"\n",
    "ci = controller.Car_Interface(model = CAR_MODEL)\n",
    "\n",
    "ci.set_gear(ci.FORWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(181)\n",
    "\n",
    "# Accelerates the given car interface until it arrives at the target velocity\n",
    "def accelerate_until_velocity(ci, target_vel):\n",
    "    while(len(ci.log[\"velocity\"]) == 0 or ci.log[\"velocity\"][-1] < target_vel):\n",
    "        ci.apply_control(pedal = ci.ACCELERATOR, amount = 1.0)\n",
    "\n",
    "'''\n",
    "Accelerate until a randomly determined target velocity.\n",
    "Record the actual achieved velocity (which maybe a little higher)\n",
    "Then brake for enough time, so that we know we've stopped.\n",
    "The stopping distance is then the difference in the car's position\n",
    "now and when we started braking.\n",
    "'''\n",
    "def stopping_distance_gen(ci, t = 50):\n",
    "    amt = 0.15 + random.random() * 0.85\n",
    "    initial_velocity = random.random()\n",
    "\n",
    "    accelerate_until_velocity(ci, initial_velocity)\n",
    "    initial_velocity = ci.log[\"velocity\"][-1]\n",
    "    ci.zero_position()\n",
    "    ci.apply_control_for_time(pedal = ci.BRAKE, amount = amt, time = t)\n",
    "\n",
    "    stopping_distance = ci.log[\"position\"][-1]\n",
    "    inp = [initial_velocity, stopping_distance]\n",
    "\n",
    "    return amt, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt, (initial_velocity, stopping_distance) = stopping_distance_gen(ci)\n",
    "print(f\"A car moving at {initial_velocity * 100:.2f}% speed, applied {amt * 100:.2f}% brakes and stopped, after travelling {stopping_distance:.2f} distance units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytically solve for the theoretical stopping distance\n",
    "def actual_stopping_distance(initial_velocity, amt):\n",
    "    x = ci.brake_weight * amt + ci.rolling_bias\n",
    "    f = ci.friction_constant\n",
    "    v0 = initial_velocity\n",
    "    \n",
    "    if ((1-f*v0/x) < 0):\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    d = ((x/f)*np.log(1-f*v0/x) + v0)/f\n",
    "    return d\n",
    "\n",
    "'''\n",
    "Use binary search to approximate the required brake amount,\n",
    "that results in a target stopping distance given an intial velocity.\n",
    "'''\n",
    "def approximate_amount(inp, tol = 1e-5, min_amt = 0, max_amt = 1):\n",
    "    mid_amt = (min_amt + max_amt) / 2\n",
    "    if (max_amt - min_amt < 2 * tol):\n",
    "        return mid_amt\n",
    "\n",
    "    v0, stopping_distance = inp\n",
    "    if (actual_stopping_distance(v0, mid_amt) < stopping_distance):\n",
    "        return approximate_amount(inp, tol, min_amt, mid_amt)\n",
    "    else:\n",
    "        return approximate_amount(inp, tol, mid_amt, max_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (MODE == 4):\n",
    "    for i in range(3):\n",
    "            amt, (initial_velocity, stopping_distance) = stopping_distance_gen(ci)\n",
    "            pred = approximate_amount((initial_velocity, stopping_distance))\n",
    "            print(f\"Car moving at {initial_velocity * 100:.2f}%; Target Stopping Distance {stopping_distance:.2f} distance units\")\n",
    "            print(f\"Simulation Brake Amount: {amt*100:.2f}%; Closed Form Brake Amount {pred*100:.2f}%\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Network Class (a custom subclass of torch's nn module)\n",
    "class fcn(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of hidden units in first hidden layer\n",
    "        self.H_1 = 30\n",
    "        # Number of hidden units in second hidden layer\n",
    "        self.H_2 = 20\n",
    "        \n",
    "        '''\n",
    "        Weights generally [input dim, output dim] so when we multiply a vector\n",
    "        of size [input dim] by a matrix of size [input dim, output dim] we get\n",
    "        a vector of size [output dim].  The bias will have shape [output dim]\n",
    "        so we can add it to the result of the weight-vector multiplication.\n",
    "        '''\n",
    "        \n",
    "        #Weights and Biases for computing input -> first hidden layer\n",
    "        self.W_1 = nn.Parameter(torch.randn([2, self.H_1]))\n",
    "        self.B_1 = nn.Parameter(torch.randn([self.H_1]))\n",
    "\n",
    "        #Weights and Biases for computing first -> second hidden layer\n",
    "        self.W_2 = nn.Parameter(torch.randn([self.H_1, self.H_2]))\n",
    "        self.B_2 = nn.Parameter(torch.randn([self.H_2]))\n",
    "        \n",
    "        #Weights and Biases for computing second hidden layer -> output\n",
    "        self.W_3 = nn.Parameter(torch.randn([self.H_2, 1]))\n",
    "        self.B_3 = nn.Parameter(torch.randn([1]))\n",
    "\n",
    "    # Forward propogation\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x will be a vector of length 2 containing the initial velocity and desired stopping distance\n",
    "        x = torch.tensor(x, dtype = torch.float32)\n",
    "\n",
    "        # first hidden layer computation with tanh activation\n",
    "        h_1 = torch.tanh(torch.matmul(x, self.W_1) + self.B_1)\n",
    "    \n",
    "        # second hidden layer computation with tanh activation\n",
    "        h_2 = torch.tanh(torch.matmul(h_1, self.W_2) + self.B_2)\n",
    "        \n",
    "        #output computation with no activation.  We technically get a vector of length 1 so we squeeze to get value.\n",
    "        out = torch.squeeze(torch.matmul(h_2, self.W_3) + self.B_3)\n",
    "\n",
    "        '''\n",
    "        Our output is a scaled sigmoid (output range (0, 1.15)).  This helps model learn faster since all \n",
    "        desired outputs are in the range (0.15, 1).\n",
    "        '''\n",
    "        return 1.15 * torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If demonstrating or testing just load initialize FCN with learned weights\n",
    "if (MODE == 1 or MODE == 3):\n",
    "    FN = \"weights_\" + CAR_MODEL\n",
    "\n",
    "    model = fcn()\n",
    "    model.load_state_dict(torch.load(open(FN + \".pt\", \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training (not we may start from previously learned weights)\n",
    "if (MODE == 0):\n",
    "    # Number of batches of data in a \"epoch\"\n",
    "    NUM_BATCHES = 10\n",
    "    # Number of data points in a single batch\n",
    "    BATCH_SIZE = 30\n",
    "    '''\n",
    "    Number of epochs. Note generally an epoch is a pass through the whole dataset.  \n",
    "    Here we are artifically generated data, so the size of an epoch is artificial as well.\n",
    "    It will be (NUM_BATCHES * BATCH_SIZE).\n",
    "    '''\n",
    "    EPOCHS = 10\n",
    "\n",
    "    # Do we want to use previously trained weights?\n",
    "    USE_LAST = True\n",
    "\n",
    "    FN = \"weights_\" + CAR_MODEL\n",
    "\n",
    "    model = fcn()\n",
    "    if(USE_LAST):\n",
    "        model.load_state_dict(torch.load(open(FN + \".pt\", \"rb\")))\n",
    "\n",
    "    '''\n",
    "    Adam is an improved version of standard Stochastic Gradient Descent.\n",
    "    The Stochastic refers to the fact that we update weights based on\n",
    "    a small subset of our overall dataset, rather than computing the gradient\n",
    "    over the entire dataset which can very time consuming.  Typically small\n",
    "    batches represent the overall data patterns in the long run.\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        # epoch loss\n",
    "        e_loss = 0\n",
    "        for b in range(NUM_BATCHES):\n",
    "            # batch loss\n",
    "            b_loss = 0\n",
    "            for i in range(BATCH_SIZE):\n",
    "                # Generate a data point\n",
    "                amt, inp = stopping_distance_gen(ci)\n",
    "                \n",
    "                # Find the model's predicted brake amount\n",
    "                out = model(inp)\n",
    "                \n",
    "                # Compute MSE between model output and actual\n",
    "                amt_t = torch.tensor(amt)\n",
    "                b_loss += (out - amt_t) ** 2\n",
    "\n",
    "            b_loss /= BATCH_SIZE\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.zero_grad()\n",
    "            b_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            b_loss = b_loss.detach().numpy()\n",
    "            print(f\"B {b} L {b_loss:.4f}\", end = \"\\r\")\n",
    "            e_loss += b_loss\n",
    "\n",
    "        e_loss /= NUM_BATCHES\n",
    "        print(f\"EPOCH {e + 1} {e_loss:.4f}\")\n",
    "        \n",
    "        # save weights\n",
    "        torch.save(model.state_dict(), open(FN + \".pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate model's effectiveness on a few generated data points\n",
    "if (MODE == 1 or MODE == 2):\n",
    "    DEM = 5\n",
    "\n",
    "    for i in range(DEM):\n",
    "\n",
    "        amt, inp = stopping_distance_gen(ci)\n",
    "\n",
    "        if (MODE == 1):\n",
    "            out = model(inp).detach().numpy()\n",
    "        elif (MODE == 2):\n",
    "            out = approximate_amount(inp, tol = 1e-5)\n",
    "\n",
    "        print(f\"INIT VEL: {inp[0]:.3f} TARG SD: {inp[1]:.3f} BRK AMT: {amt:.3f} MODEL OUT:{out:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models over a larger set of datapoints, plot error distribution.\n",
    "if (MODE == 3 or MODE == 4):\n",
    "    TEST = 300\n",
    "    correct = 0\n",
    "    tol = 0.1\n",
    "    errors = []\n",
    "    for i in range(TEST):\n",
    "        print(f\"TESTING {i + 1}/{TEST}\", end = \"\\r\")\n",
    "        amt, inp = stopping_distance_gen(ci)\n",
    "\n",
    "        if (MODE == 3):\n",
    "            out = model(inp).detach().numpy()\n",
    "        elif (MODE == 4):\n",
    "            out = approximate_amount(inp, tol = 1e-5)\n",
    "\n",
    "        if (abs(out - amt) < tol):\n",
    "            correct += 1\n",
    "        errors.append(out - amt)\n",
    "    print(f\"WITHIN {tol} {correct}/{TEST} times\")\n",
    "    print(f\"AVERAGE ERROR {np.mean(np.abs(errors))}\")\n",
    "\n",
    "    plt.title(\"Error Distribution\")\n",
    "    plt.hist(errors, bins = 200, range = (-1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
